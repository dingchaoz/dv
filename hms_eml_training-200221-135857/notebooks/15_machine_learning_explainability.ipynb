{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# USE CASES FOR MODEL INSIGHTS\n\n### What Types of Insights Are Possible\n\nhttps://christophm.github.io/interpretable-ml-book/\n\nMany people say machine learning models are \"black boxes\", in the sense that they they can make good predictions but you can't understand the logic behind those predictions. This statement is true in the sense that most data scientists don't know how to extract insights from models yet.\n\nHowever, this short session will teach you techniques to explain the findings from sophisticated machine learning models.\n\n* What features in the data did the model think are most important?\n* For any single prediction from a model, how did each feature in the data affect that particular prediction\n* How does each feature affect the model's predictions in a big-picture sense (what is its typical effect when considered over a large number of possible predictions?).\n\n### Why Are These Insights Valuable\n\nThese insights have many uses, including\n\n- Debugging\n- Informing feature engineering\n- Directing future data collection\n- Informing human decision-making\n- Building Trust\n\n### Debugging\n\nThe world has a lot of unreliable, disorganized and generally dirty data. You add a potential source of errors as you write preprocessing code. Add in the potential for target leakage and it is the norm rather than the exception to have errors at some point in a real data science projects.\n\nGiven the frequency and potentially distastrous consequences of bugs, debugging is one of the most valuable skills in data science. Understanding the patterns a model is finding will help you identify when those are at odds with your knowledge of the real world, and this is typically the first step in tracking down bugs.\n\n### Informing Feature Engineering\n\nFeature engineering is usually the most effective way to improve model accuracy. Feature engineering usually involves repeatedly creating new features using transformations of your raw data or features you have previously created.\n\nSometimes you can go through this process using nothing but intuition about the underlying topic. But you'll need more direction when you have 100s of raw features or when you lack background knowledge about the topic you are working on.\n\nA Kaggle competition to [predict loan defaults](https://www.kaggle.com/c/loan-default-prediction) gives an extreme example. This competition had 100s of raw features. For privacy reasons, the features had names like f1, f2, f3 rather than common English names. This simulated a scenario where you have little intuition about the raw data.\n\nOne competitor found that the difference between two of the features, specificallyf527 - f528, created a very powerful new feature. Models including that difference as a feature were far better than models without it. But how might you think of creating this variable when you start with hundreds of variables?\n\nThe techniques you'll learn in this session would make it transparent that f527 and f528 are important features, and that their role is tightly entangled. This will direct you to consider transformations of these two variables, and likely find the \"golden feature\" of f527 - f528.\n\nAs an increasing number of datasets start with 100s or 1000s of raw features, this approach is becoming increasingly important.\n\n### Directing Future Data Collection\n\nYou have no control over datasets you download online. But many businesses and organizations using data science have opportunities to expand what types of data they collect. Collecting new types of data can be expensive or inconvenient, so they only want to do this if they know it will be worthwhile. Model-based insights give you a good understanding of the value of features you currently have, which will help you reason about what new values may be most helpful.\n\n### Informing Human Decision-Making\n\nSome decisions are made automatically by models. Amazon doesn't have humans (or elves) scurry to decide what to show you whenever you go to their website. But many important decisions are made by humans. For these decisions, insights can be more valuable than predictions.\n\n### Building Trust\n\nMany people won't assume they can trust your model for important decisions without verifying some basic facts. This is a smart precaution given the frequency of data errors. In practice, showing insights that fit their general understanding of the problem will help build trust, even among people with little deep knowledge of data science."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## PERMUTATION IMPORTANCE\n\nOne of the most basic questions we might ask of a model is What features have the biggest impact on predictions?\n\nThis concept is called feature importance. We've seen feature importance used effectively many times for every purpose in the use cases list.\n\nThere are multiple ways to measure feature importance. Some approaches answer subtly different versions of the question above. Other approaches have documented shortcomings.\n\nThis lesson focuses on permutation importance. Compared to most other approaches, permutation importance is:\n\n- Fast to calculate\n- Widely used and understood\n- Consistent with properties we would want a feature importance measure to have\n\n\n### How it Works\n\nPermutation importance uses models differently than anything you've seen so far, and many people find it confusing at first. So we'll start with an example to make it more concrete.\n\nConsider data with the following format:\n\n<img src='./images/permutation-imp1.png' />\n\nWe want to predict a person's height when they become 20 years old, using data that is available at age 10.\n\nOur data includes useful features (height at age 10), features with little predictive power (socks owned), as well as some other features we won't focus on in this explanation.\n\nPermutation importance is calculated after a model has been fitted. So we won't change the model or change what predictions we'd get for a given value of height, sock-count, etc.\n\nInstead we will ask the following question: If we randomly shuffle a single column of the validation data, leaving the target and all other columns in place, how would that affect the accuracy of predictions in that now-shuffled data?\n\n<img src='./images/permutation-imp2.png' />\n\nRandomly re-ordering a single column should cause less accurate predictions, since the resulting data no longer corresponds to anything observed in the real world. Model accuracy especially suffers if we shuffle a column that the model relied on heavily for predictions. In this case, shuffling height at age 10 would cause terrible predictions. If we shuffled socks owned instead, the resulting predictions wouldn't suffer nearly as much.\n\nWith this insight, the process is as follows:\n\n- Get a trained model\n- Shuffle the values in a single column, make predictions using the resulting dataset. Use these predictions and the true target values to calculate how much the loss function suffered from shuffling. That performance deterioration measures the importance of the variable you just shuffled.\n- Return the data to the original order (undoing the shuffle from step 2.) Now repeat step 2 with the next column in the dataset, until you have calculated the importance of each column."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Code Example\n\nOur example will use a model that predicts whether a soccer/football team will have the \"Man of the Game\" winner based on the team's statistics. The \"Man of the Game\" award is given to the best player in the game. Model-building isn't our current focus, so the cell below loads the data and builds a rudimentary model."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport warnings\n#warnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\")",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "data = pd.read_csv('../data/FIFA 2018 Statistics.csv')",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "data.info()",
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 128 entries, 0 to 127\nData columns (total 27 columns):\nDate                      128 non-null object\nTeam                      128 non-null object\nOpponent                  128 non-null object\nGoal Scored               128 non-null int64\nBall Possession %         128 non-null int64\nAttempts                  128 non-null int64\nOn-Target                 128 non-null int64\nOff-Target                128 non-null int64\nBlocked                   128 non-null int64\nCorners                   128 non-null int64\nOffsides                  128 non-null int64\nFree Kicks                128 non-null int64\nSaves                     128 non-null int64\nPass Accuracy %           128 non-null int64\nPasses                    128 non-null int64\nDistance Covered (Kms)    128 non-null int64\nFouls Committed           128 non-null int64\nYellow Card               128 non-null int64\nYellow & Red              128 non-null int64\nRed                       128 non-null int64\nMan of the Match          128 non-null object\n1st Goal                  94 non-null float64\nRound                     128 non-null object\nPSO                       128 non-null object\nGoals in PSO              128 non-null int64\nOwn goals                 12 non-null float64\nOwn goal Time             12 non-null float64\ndtypes: float64(3), int64(18), object(6)\nmemory usage: 27.1+ KB\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "data.shape",
      "execution_count": 10,
      "outputs": [
        {
          "data": {
            "text/plain": "(128, 27)"
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "for i in data.columns:\n    if data[i].dtype in [np.int64]:\n        print (i)",
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Goal Scored\nBall Possession %\nAttempts\nOn-Target\nOff-Target\nBlocked\nCorners\nOffsides\nFree Kicks\nSaves\nPass Accuracy %\nPasses\nDistance Covered (Kms)\nFouls Committed\nYellow Card\nYellow & Red\nRed\nGoals in PSO\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "y = (data['Man of the Match'] == \"Yes\")  # Convert from string \"Yes\"/\"No\" to binary\nfeature_names = [i for i in data.columns if data[i].dtype in [np.int64]]\nX = data[feature_names]\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\nmodel = RandomForestClassifier(random_state=0).fit(X_train, y_train)",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Here is how to calculate and show importances with the [eli5](https://eli5.readthedocs.io/en/latest/) library:"
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "! pip install eli5",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": " X_test.columns.tolist()",
      "execution_count": 8,
      "outputs": [
        {
          "data": {
            "text/plain": "['Goal Scored',\n 'Ball Possession %',\n 'Attempts',\n 'On-Target',\n 'Off-Target',\n 'Blocked',\n 'Corners',\n 'Offsides',\n 'Free Kicks',\n 'Saves',\n 'Pass Accuracy %',\n 'Passes',\n 'Distance Covered (Kms)',\n 'Fouls Committed',\n 'Yellow Card',\n 'Yellow & Red',\n 'Red',\n 'Goals in PSO']"
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "X_test.shape",
      "execution_count": 11,
      "outputs": [
        {
          "data": {
            "text/plain": "(32, 18)"
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(model, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())",
      "execution_count": 9,
      "outputs": [
        {
          "data": {
            "text/html": "\n    <style>\n    table.eli5-weights tr:hover {\n        filter: brightness(85%);\n    }\n</style>\n\n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n    <thead>\n    <tr style=\"border: none;\">\n        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n    </tr>\n    </thead>\n    <tbody>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0750\n                \n                    &plusmn; 0.1159\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                Goal Scored\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 82.40%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0625\n                \n                    &plusmn; 0.0791\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                Corners\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 86.29%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0437\n                \n                    &plusmn; 0.0500\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                Distance Covered (Kms)\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 87.69%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0375\n                \n                    &plusmn; 0.0729\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                On-Target\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 87.69%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0375\n                \n                    &plusmn; 0.0468\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                Free Kicks\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 92.42%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0187\n                \n                    &plusmn; 0.0306\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                Blocked\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 94.29%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0125\n                \n                    &plusmn; 0.0750\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                Pass Accuracy %\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 94.29%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0125\n                \n                    &plusmn; 0.0500\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                Yellow Card\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 96.49%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0063\n                \n                    &plusmn; 0.0468\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                Saves\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 96.49%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0063\n                \n                    &plusmn; 0.0250\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                Offsides\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 96.49%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0063\n                \n                    &plusmn; 0.1741\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                Off-Target\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(0, 100.00%, 100.00%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0000\n                \n                    &plusmn; 0.1046\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                Passes\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(0, 100.00%, 100.00%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0\n                \n                    &plusmn; 0.0000\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                Red\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(0, 100.00%, 100.00%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0\n                \n                    &plusmn; 0.0000\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                Yellow &amp; Red\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(0, 100.00%, 100.00%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0\n                \n                    &plusmn; 0.0000\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                Goals in PSO\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(0, 100.00%, 89.16%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                -0.0312\n                \n                    &plusmn; 0.0884\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                Fouls Committed\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(0, 100.00%, 87.69%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                -0.0375\n                \n                    &plusmn; 0.0919\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                Attempts\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(0, 100.00%, 84.94%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                -0.0500\n                \n                    &plusmn; 0.0500\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                Ball Possession %\n            </td>\n        </tr>\n    \n    \n    </tbody>\n</table>\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n\n",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Interpreting Permutation Importances\n\nThe values towards the top are the most important features, and those towards the bottom matter least.\n\nThe first number in each row shows how much model performance decreased with a random shuffling (in this case, using \"accuracy\" as the performance metric).\n\nLike most things in data science, there is some randomness to the exact performance change from a shuffling a column. We measure the amount of randomness in our permutation importance calculation by repeating the process with multiple shuffles. The number after the ± measures how performance varied from one-reshuffling to the next.\n\nYou'll occasionally see negative values for permutation importances. In those cases, the predictions on the shuffled (or noisy) data happened to be more accurate than the real data. This happens when the feature didn't matter (should have had an importance close to 0), but random chance caused the predictions on shuffled data to be more accurate. This is more common with small datasets, like the one in this example, because there is more room for luck/chance.\n\nIn this example, the most important feature was **Goals scored**. That seems sensible. Soccer fans may have some intuition about whether the orderings of other variables are surprising or not."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}