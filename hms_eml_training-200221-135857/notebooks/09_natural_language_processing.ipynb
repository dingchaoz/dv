{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# NATURAL LANGUAGE PROCESSING"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## PART 1: INTRODUCTION\n*Adapted from [NLP Crash Course](http://files.meetup.com/7616132/DC-NLP-2013-09%20Charlie%20Greenbacker.pdf) by Charlie Greenbacker and [Introduction to NLP](http://spark-public.s3.amazonaws.com/nlp/slides/intro.pdf) by Dan Jurafsky*\n\n## What is NLP?\n- Using computers to process (analyze, understand, generate) natural human languages\n\n## Why is NLP useful?\n- Most knowledge created by humans is unstructured text\n- Need some way to make sense of it\n- Enables quantitative analysis of text data\n\n## What are some of the higher level task areas?\n- **Speech recognition and generation**: Apple Siri\n    - Speech to text\n    - Text to speech\n- **Question answering**: IBM Watson\n    - Match query with knowledge base\n    - Reasoning about intent of question\n- **Machine translation**: Google Translate\n    - One language to another to another\n- **Information retrieval**: Google\n    - Finding relevant results\n    - Finding similar results\n- **Information extraction**: Gmail\n    - Structured information from unstructured documents\n- **Assistive technologies**: Google autocompletion\n    - Predictive text input\n    - Text simplification\n- **Natural Language Generation**: computer-generated articles\n    - Generating text from data\n- **Automatic summarization**: Google News\n    - Extractive summarization\n    - Abstractive summarization\n- **Sentiment analysis**: Twitter analysis\n    - Attitude of speaker\n\n## What are some of the lower level components?\n- **Tokenization**: breaking text into tokens (words, sentences, n-grams)\n- **Stopword removal**: a/an/the\n- **Stemming and lemmatization**: root word\n- **TF-IDF**: word importance\n- **Part-of-speech tagging**: noun/verb/adjective\n- **Named entity recognition**: person/organization/location\n- **Spelling correction**: \"New Yrok City\"\n- **Word sense disambiguation**: \"buy a mouse\"\n- **Segmentation**: \"New York City subway\"\n- **Language detection**: \"translate this page\"\n- **Machine learning**\n\n## Why is NLP hard?\n- **Ambiguity**:\n    - Teacher Strikes Idle Kids\n    - Red Tape Holds Up New Bridges\n    - Hospitals are Sued by 7 Foot Doctors\n    - Juvenile Court to Try Shooting Defendant\n    - Local High School Dropouts Cut in Half\n- **Non-standard English**: tweets/text messages\n- **Idioms**: \"throw in the towel\"\n- **Newly coined words**: \"retweet\"\n- **Tricky entity names**: \"Where is A Bug's Life playing?\"\n- **World knowledge**: \"Mary and Sue are sisters\", \"Mary and Sue are mothers\"\n\n## How does NLP work?\n- Build probabilistic model using data about a language\n- Requires an understanding of the language\n- Requires an understanding of the world (or a particular domain)\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## PART 2: READING IN THE YELP REVIEWS"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "- \"corpus\" = collection of documents\n- \"corpora\" = plural form of corpus"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "## PRE-REQUISITES (Install the following from the Terminal)\n## pip install textblob\n## python -m textblob.download_corpora",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "! pip install textblob ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "! python -m textblob.download_corpora -y",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport scipy as sp\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom textblob import TextBlob, Word\nfrom nltk.stem.snowball import SnowballStemmer",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import sys\n\n# Check for python version\nreq_version = (2,5)\ncur_version = sys.version_info",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# read yelp.csv into a DataFrame\nyelp = pd.read_csv('../data/yelp.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "yelp.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "yelp.stars.value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# create a new DataFrame that only contains the 5-star and 1-star reviews\nyelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "yelp_best_worst.stars.value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "yelp_best_worst.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "yelp_best_worst.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "yelp_best_worst.stars.value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# split the new DataFrame into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(yelp_best_worst.text, yelp_best_worst.stars, random_state=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## PART 3: TOKENIZATION\n- **What:** Separate text into units such as sentences or words\n- **Why:** Gives structure to previously unstructured text\n- **Notes:** Relatively easy with English language text, not easy with some languages"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# use CountVectorizer to create document-term matrices from X_train and X_test\nvect = CountVectorizer()\ntrain_dtm = vect.fit_transform(X_train)\ntest_dtm = vect.transform(X_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# rows are documents, columns are terms (aka \"tokens\" or \"features\")\ntrain_dtm.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# last 50 features\nprint (vect.get_feature_names()[-50:])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# show vectorizer options\nvect",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**[CountVectorizer documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)**\n- **lowercase:** boolean, True by default\n- Convert all characters to lowercase before tokenizing."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# don't convert to lowercase\nvect = CountVectorizer(lowercase=False)\ntrain_dtm = vect.fit_transform(X_train)\ntrain_dtm.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "- **token_pattern:** string\n- Regular expression denoting what constitutes a \"token\". The default regexp select tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# allow tokens of one character\nvect = CountVectorizer(token_pattern=r'(?u)\\b\\w+\\b')\ntrain_dtm = vect.fit_transform(X_train)\ntrain_dtm.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print (vect.get_feature_names()[-50:])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "- **ngram_range:** tuple (min_n, max_n)\n- The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# include 1-grams and 2-grams\nvect = CountVectorizer(ngram_range=(1, 2))\ntrain_dtm = vect.fit_transform(X_train)\ntrain_dtm.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print (vect.get_feature_names()[-50:])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "sw = ['in', 'on', 'the']\nvect = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\" )\ntrain_dtm = vect.fit_transform(X_train)\ntrain_dtm.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "vect.get_stop_words()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# last 50 features\nprint (vect.get_feature_names()[-50:])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### `PREDICTING THE STAR RATING`"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# use default options for CountVectorizer\nvect = CountVectorizer()\nvect.fit(X_train)\n\n# create document-term matrices. In this case, we are transforming X_test tokens to fit the vocabulary generated by X_train\ntrain_dtm = vect.transform(X_train)\ntest_dtm = vect.transform(X_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print (train_dtm.shape)\nprint (test_dtm.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# You get different set of words counts when you fit and transform train and test sets separately\ntrain_vect = CountVectorizer()\ntrain_vect.fit(X_train)\ntrain_dtm2 = train_vect.transform(X_train)\nprint (train_dtm2.shape)\n\ntest_vect = CountVectorizer()\ntest_vect.fit(X_test)\ntest_dtm2 = test_vect.transform(X_test)\nprint (test_dtm2.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# use Naive Bayes to predict the star rating\nnb = MultinomialNB()\nnb.fit(train_dtm, y_train)\ny_pred_class = nb.predict(test_dtm)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X_train\nvect.get_feature_names()[-50:]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "y_test",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# calculate accuracy\nprint (metrics.accuracy_score(y_test, y_pred_class))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# calculate null accuracy\ny_test_binary = np.where(y_test==5, 1, 0)\ny_test_binary.mean()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# define a function that accepts a vectorizer and returns the accuracy\ndef tokenize_test(vect):\n    train_dtm = vect.fit_transform(X_train)\n    print ('Features: ', train_dtm.shape[1])\n    test_dtm = vect.transform(X_test)\n    nb = MultinomialNB()\n    nb.fit(train_dtm, y_train)\n    y_pred_class = nb.predict(test_dtm)\n    print ('Accuracy: ', metrics.accuracy_score(y_test, y_pred_class))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# include 1-grams and 2-grams\nvect = CountVectorizer(ngram_range=(1, 3))\ntokenize_test(vect)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "vect.get_feature_names()[-50:]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## PART 4: STOPWORD REMOVAL\n- **What:** Remove common words that will likely appear in any text\n- **Why:** They don't tell you much about your text"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# show vectorizer options\nvect",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "- **stop_words:** string {'english'}, list, or None (default)\n- If 'english', a built-in stop word list for English is used.\n- If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n- If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# remove English stop words\nvect = CountVectorizer(stop_words='english')\ntokenize_test(vect)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# set of stop words\nprint (vect.get_stop_words())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## PART 5: OTHER COUNTVECTORIZER OPTIONS \n- **max_features:** int or None, default=None\n- If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# remove English stop words and only keep 100 features\nvect = CountVectorizer(stop_words='english', max_features=100)\ntokenize_test(vect)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_dtm = vect.fit_transform(X_train)\npd.DataFrame(train_dtm.toarray(), columns=vect.get_feature_names())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# all 100 features\nprint (vect.get_feature_names())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# include 1-grams and 2-grams, and limit the number of features\nvect = CountVectorizer(ngram_range=(1, 2), max_features=100000)\ntokenize_test(vect)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "- **min_df:** float in range [0.0, 1.0] or int, default=1\n- When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# include 1-grams and 2-grams, and only include terms that appear at least 2 times\nvect = CountVectorizer(ngram_range=(1, 2), min_df=2)\ntokenize_test(vect)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#### confirm the code tomorrow\nvect = CountVectorizer(ngram_range=(1, 2), min_df=2)\ntrain_dtm = vect.fit_transform(X_train)\npd.DataFrame(train_dtm.toarray(), columns=vect.get_feature_names())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Part 6: INTRODUCTION TO TextBlob\n* TextBlob: \"Simplified Text Processing\""
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# print the first review\nprint (yelp_best_worst.text[0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# save it as a TextBlob object\nreview = TextBlob(yelp_best_worst.text[0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# list the words\nreview.words",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "len(review.words)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# list the sentences\nreview.sentences",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# some string methods are available\nreview.lower()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "review.noun_phrases",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## PART 7: STEMMING AND LEMMATIZATION"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**STEMMING:**\n- **What:** Reduce a word to its base/stem/root form\n- **Why:** Often makes sense to treat related words the same way\n- **Notes:**\n    - Uses a \"simple\" and fast rule-based approach\n    - Stemmed words are usually not shown to users (used for analysis/indexing)\n    - Some search engines treat words with the same stem as synonyms"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# initialize stemmer\nstemmer = SnowballStemmer('english')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# stem each word\nstems = [stemmer.stem(word) for word in review.words]\nprint (len(stems))\nprint (review.words)\nprint (stems)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print (len(stems))\nprint (len(set(stems)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**LEMMATIZATION**\n- **What:** Derive the canonical form ('lemma') of a word\n- **Why:** Can be better than stemming\n- **Notes:** Uses a dictionary-based approach (slower than stemming)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# assume every word is a noun\nlems = [word.lemmatize() for word in review.words]\nprint (len(lems))\nprint (review.words)\nprint (set(lems))\nprint (len(set(lems)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# assume every word is a verb\nvlems = [word.lemmatize() for word in review.words]\nprint (len(vlems))\nprint (len(set(vlems)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# define a function that accepts text and returns a list of lemmas\ndef split_into_lemmas(text):\n    if cur_version >= req_version:\n        text = text.lower()\n    else:\n        text = unicode(text, 'utf-8').lower()\n        \n    words = TextBlob(text).words\n    return [word.lemmatize() for word in words]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# use split_into_lemmas as the feature extraction function\nvect = CountVectorizer(analyzer=split_into_lemmas)\ntokenize_test(vect)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# last 50 features\nprint (vect.get_feature_names()[-50:])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## PART 8: TERM FREQUENCY - INVERSE DOCUMENT FREQUENCY \n- **What:** Computes \"relative frequency\" that a word appears in a document compared to its frequency across all documents\n- **Why:** More useful than \"term frequency\" for identifying \"important\" words in each document (high frequency in that document, low frequency in other documents)\n- **Notes:** Used for search engine scoring, text summarization, document clustering"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# example documents\ntrain_simple = ['call you tonight',\n                'Call me a cab',\n                'please call me... PLEASE!']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# CountVectorizer\nvect = CountVectorizer()\npd.DataFrame(vect.fit_transform(train_simple).toarray(), columns=vect.get_feature_names())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# TfidfVectorizer\nvect = TfidfVectorizer()\npd.DataFrame(vect.fit_transform(train_simple).toarray(), columns=vect.get_feature_names())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## PART 9: USING TF-IDF TO SUMMARIZE A YELP REVIEW"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# create a document-term matrix using TF-IDF\nvect = TfidfVectorizer(stop_words='english')\ndtm = vect.fit_transform(yelp.text)\nfeatures = vect.get_feature_names()\ndtm.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def summarize():\n    \n    # choose a random review that is at least 300 characters\n    review_length = 0\n    while review_length < 300:\n        review_id = np.random.randint(0, len(yelp))\n        review_text = \"\"\n        review_length = 0\n        \n        if cur_version >= req_version:\n            review_text = yelp.text[review_id]\n            review_length = len(review_text)\n        else:\n            # Python version 2.7\n            review_text = unicode(yelp.text[review_id], 'utf-8')\n            review_length = len(review_text)\n\n    # create a dictionary of words and their TF-IDF scores\n    word_scores = {}\n    for word in TextBlob(review_text).words:\n        word = word.lower()\n        if word in features:\n            word_scores[word] = dtm[review_id, features.index(word)]\n    \n    # print words with the top 5 TF-IDF scores\n    print ('TOP SCORING WORDS:')\n    top_scores = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)[:5]\n    for word, score in top_scores:\n        print (word)\n    \n    # print 5 random words\n    print ('RANDOM WORDS:')\n    \n    if cur_version >= req_version:\n        random_words = np.random.choice(list(word_scores.keys()), size=5, replace=False)\n    else:\n        random_words = np.random.choice(word_scores.keys(), size=5, replace=False)\n        \n    for word in random_words:\n        print (word)\n    \n    # print the review\n    print (review_text)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "summarize()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## PART 10: SENTIMENT ANALYSIS"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print (review)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# polarity ranges from -1 (most negative) to 1 (most positive)\nreview.sentiment.polarity",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "review.translate(to=\"es\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "yelp.columns",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# understanding the apply method\nyelp['length'] = yelp.text.apply(len)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# define a function that accepts text and returns the polarity\ndef detect_sentiment(text):\n    blob = None\n    if cur_version >= req_version:\n        blob = TextBlob(text).sentiment.polarity\n    else:\n        blob = TextBlob(text.decode('utf-8')).sentiment.polarity\n    return blob",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# create a new DataFrame column for sentiment\nyelp['sentiment'] = yelp.text.apply(detect_sentiment)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "yelp.columns",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\n\n# boxplot of sentiment grouped by stars\nyelp.boxplot(column='sentiment', by='stars')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# reviews with most positive sentiment\nyelp[yelp.sentiment == 1].text.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# reviews with most negative sentiment\nyelp[yelp.sentiment == -1].text.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# widen the column display\npd.set_option('max_colwidth', 500)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# negative sentiment in a 5-star review\nyelp[(yelp.stars == 5) & (yelp.sentiment < -0.3)].head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# positive sentiment in a 1-star review\nyelp[(yelp.stars == 1) & (yelp.sentiment > 0.5)].head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# reset the column display width\npd.reset_option('max_colwidth')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## PART 11: ADDING FEATURES TO A DOCUMENT-TERM MATRIX"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# create a new DataFrame that only contains the 5-star and 1-star reviews\nyelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n\n# split the new DataFrame into training and testing sets\nfeature_cols = ['text', 'sentiment', 'cool', 'useful', 'funny']\nX = yelp_best_worst[feature_cols]\ny = yelp_best_worst.stars\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X_train.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# use CountVectorizer with text column only\nvect = CountVectorizer()\ntrain_dtm = vect.fit_transform(X_train.iloc[:, 0])\ntest_dtm = vect.transform(X_test.iloc[:, 0])\nprint (train_dtm.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# cast other feature columns to float and convert to a sparse matrix\nextra = sp.sparse.csr_matrix(X_train.iloc[:, 1:].astype(float))\nextra.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "extra",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# combine sparse matrices\ntrain_dtm_extra = sp.sparse.hstack((train_dtm, extra))\ntrain_dtm_extra.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# repeat for testing set\nextra = sp.sparse.csr_matrix(X_test.iloc[:, 1:].astype(float))\ntest_dtm_extra = sp.sparse.hstack((test_dtm, extra))\ntest_dtm_extra.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# use logistic regression with text column only\nlogreg = LogisticRegression(C=1e9)\nlogreg.fit(train_dtm, y_train)\ny_pred_class = logreg.predict(test_dtm)\nprint (metrics.accuracy_score(y_test, y_pred_class))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# use logistic regression with all features\nlogreg = LogisticRegression(C=1e9)\nlogreg.fit(train_dtm_extra, y_train)\ny_pred_class = logreg.predict(test_dtm_extra)\nprint (metrics.accuracy_score(y_test, y_pred_class))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## PART 12: SAVE AND LOAD YOUR MODEL"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from nltk.corpus import stopwords\nfrom nltk.tokenize import wordpunct_tokenize",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "try:\n    import cPickle as pickle\nexcept ImportError:\n    import pickle\n\n# create a new DataFrame that only contains the 5-star and 1-star reviews\nyelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n\n# split the new DataFrame into training and testing sets\nfeature_cols = ['text']\nX = yelp_best_worst[feature_cols]\ny = yelp_best_worst.stars\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n\n# use CountVectorizer with text column only\nvect = CountVectorizer()\ntrain_dtm = vect.fit_transform(X_train.iloc[:, 0])\ntest_dtm = vect.transform(X_test.iloc[:, 0])\n\n# use logistic regression with text column only\nlogreg = LogisticRegression(C=1e9)\nlogreg.fit(train_dtm, y_train)\ny_pred_class = logreg.predict(test_dtm)\nprint (metrics.accuracy_score(y_test, y_pred_class))\n\n## Dump the Logistic Regression Model\nout_s = open('yelp_nlp_logreg.pkl', 'wb')\npickle.dump(logreg, out_s)\nout_s.close()\n\n## Save vocabulary\nngram_size = 1\nvectorizer = CountVectorizer(ngram_range=(ngram_size, ngram_size), min_df=1)\nvect = vectorizer.fit(X_train.iloc[:, 0])\n\ndictionary_filepath = 'yelp_nlp_vocabulary.pkl'\npickle.dump(vect.vocabulary_, open(dictionary_filepath, 'wb'))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "try:\n    import cPickle as pickle\nexcept ImportError:\n    import pickle\n    \ndef classify_with_logreg_model(review):\n    ## TESTING WITH LOGISTIC REGRESSION & BAG-OF-WORDS\n    dictionary_filepath = 'yelp_nlp_vocabulary.pkl'\n    model_filepath = 'yelp_nlp_logreg.pkl'\n\n    # LOAD VOCABULARY\n    vocabulary_to_load = pickle.load(open(dictionary_filepath, 'rb'))\n    loaded_vectorizer = CountVectorizer(ngram_range=(ngram_size, ngram_size), min_df=1, vocabulary=vocabulary_to_load)\n    loaded_vectorizer._validate_vocabulary()\n\n    ## LOAD THE SAVED CLASSIFIER\n    in_logreg = open(model_filepath, 'rb')\n    classifier = pickle.load(in_logreg)\n    in_logreg.close()\n    \n    review_counts = loaded_vectorizer.fit_transform([review]).toarray()\n    predictions = classifier.predict(review_counts)\n    return predictions",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "yelp_best_worst.text[0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "yelp_best_worst.text[35]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print (classify_with_logreg_model(yelp_best_worst.text[0]))\nprint (classify_with_logreg_model(yelp_best_worst.text[35]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "some_text = \"Addison Londoner is crazy fun during soccer matches... but not so awful for everyday lunches\"\nclassify_with_logreg_model(some_text)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## PART 13: FUN TEXTBLOB FEATURES"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# spelling correction\nTextBlob('15 minuets late').correct()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# spellcheck\nWord('parot').spellcheck()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# definitions\nWord('bank').define('v')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import spacy\n\n# Load English tokenizer, tagger, parser, NER and word vectors\nnlp = spacy.load(\"en_core_web_sm\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "doc = nlp(yelp_best_worst.text[35])\n\n# Analyze syntax\nprint(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\nprint(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n\n# Find named entities, phrases and concepts\nfor entity in doc.ents:\n    print(entity.text, entity.label_)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "widgets": {
      "state": {},
      "version": "1.1.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}