{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# CLUSTERING WITH DBSCAN\n\nCluster Analysis is an important problem in data analysis. Data scientists use clustering to identify malfunctioning servers, group genes with similar expression patterns, or various other applications.\n\nK-Means determines `k` centroids in the data and clusters points by assigning them to the nearest centroid. While K-Means is easy to understand and implement in practice, the algorithm has no notion of outliers, so all points are assigned to a cluster even if they do not belong in any. In the domain of anomaly detection, this causes problems as anomalous points will be assigned to the same cluster as \"normal\" data points. The anomalous points pull the cluster centroid towards them, making it harder to classify them as anomalous points.\n\nCompared to centroid-based clustering like K-Means, density-based clustering works by identifying \"dense\" clusters of points, allowing it to learn clusters of arbitrary shape and identify outliers in the data. \n\n## Preliminary: ɛ-Balls and neighborhood density\n\nBefore we can discuss density-based clustering, we first need to cover a topic that you may have seen in a topology course: ɛ-neighborhoods.\n\nThe general idea behind ɛ-neighborhoods is given a data point, we want to be able to discover the data points in the space around it. Formally, for some real-valued ɛ > 0 and some point p, the ɛ-neighborhood of p is defined as the set of points that are at most distance ɛ away from p.\n\nIf you think back to geometry, the shape in which all points are equidistant from the center is the circle. In 2D space, the ɛ-neighborhood of a point p is the set of points contained in a circle of radius ɛ, centered at p. In 3D space, the ɛ-neighborhood is a sphere of radius ɛ, centered at p, and in higher dimensional space, the ɛ-neighborhood is just the N-sphere of radius ɛ, centered at p.\n\nLet's consider an example to make this idea more concrete - scattered 100 data points in the interval [1,3] X [2,4]. Let's pick the point (3,2) to be our point p."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np\nfrom scipy.spatial.distance import cdist\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Epsilon-Balls"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "test_pt = np.array([3,2])\ndatapoints = 1.5 * (np.random.rand(100,2)-0.5) + test_pt",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def compute_balls(data, epsilon = 0.5):\n    circle = np.linspace(0, 2*np.pi, 100)\n    neighborhood = []\n    data_x, data_y = data\n    #Create a circle of radius epsilon\n    for direction in circle:\n        x_pos = data_x + epsilon*np.cos(direction)\n        y_pos = data_y + epsilon*np.sin(direction)\n        neighborhood.append((x_pos,y_pos))\n    return neighborhood",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "neighborhood = compute_balls(test_pt, epsilon = 0.15)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "X,Y = zip(*neighborhood)\nX,Y = np.array(X),np.array(Y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "count = len(list(filter(lambda x: x < 0.15, cdist(datapoints, test_pt.reshape(1,2)))))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "plt.fill_between(X, Y, alpha = 0.25, color = 'g', label = \"Neighborhood\")\nplt.scatter(datapoints[:,0], datapoints[:,1], s = 30, color = 'r',label = \"Datapoints\")\nplt.scatter(test_pt[0],test_pt[1], s = 35, label = \"Point p\")\nplt.xlim(2,4)\nplt.ylim(1,3)\nplt.legend(loc = 2)\nplt.title(\"Density-Reachable with radius 0.15\")\n#plt.savefig(\"results/neighborhood_reachable_base.png\", format=\"PNG\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The opaque green oval represents our neighborhood, and there are 6 data points in this neighborhood. Since we scattered 100 data points and 6 are in the neighborhood, this means that a 6% of the data points are contained within the neighborhood of p with radius 0.15.\n\nNow that we have defined what we mean by a \"neighborhood\", we'll introduce the next important concept: the notion of a \"density\" for a neighborhood (we're building up to describing \"density-based clustering, after all).\n\nIn a grade-school science class, children are taught that density = mass/volume. Let's use this idea of mass divided by volume to define density at some point p. If we consider some point p and its neighborhood of radius ɛ, we can define the mass of the neighborhood as the number of data points (or alternatively, the fraction of data points) contained within the neighborhood, and the volume of the neighborhood is volume of the resulting shape of the neighborhood. In the 2D case, the neighborhood is a circle, so the volume of the neighborhood is just the area of the resulting circle. In the 3D and higher dimensional case, the neighborhood is a sphere or n-sphere, so we can calculate the volume of this shape.\n\nThe mass is the number of data points in the neighborhood, so mass = 31. The volume is the area of the circle, so volume = π0.52 = π/4. Therefore, our local density approximation at *p = (3,2) is calculated as density = mass/volume = 31/(π/4) = 124/π ~= 39.5.\n\nThis value is meaningless by itself, but if we calculate the local density approximation for all points in our dataset, we could cluster our points by saying that points that are nearby (contained in the same neighborhood) and have similar local density approximations belong in the same cluster. If we decrease the value of ɛ, we can construct smaller neighborhoods (less volume) that would also contain fewer data points. Ideally, we want to identify highly dense neighborhoods where most of the data points are contained in these neighborhoods, but the volume of each of these neighborhoods is relatively small.\n\nWhile this is not exactly what DBSCAN algorithm does, it forms the general intuition behind density-based clustering.\n\nTo recap, we discussed the ɛ-neighborhoods and how they allow us to reason about the space around a particular point. Then we defined a notion of density at a particular point for a particular neighborhood. In the next section, we'll discuss the DBSCAN algorithm where the ɛ-ball is a fundamental tool for defining clusters."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "neighborhood = compute_balls(test_pt, epsilon = 0.50)\nX,Y = zip(*neighborhood)\nX,Y = np.array(X),np.array(Y)\ncount = len(list(filter(lambda x: x < 0.15, cdist(datapoints, test_pt.reshape(1,2)))))\n\nplt.fill_between(X, Y, alpha = 0.55, color = 'g', label = \"Neighborhood\")\nplt.scatter(datapoints[:,0], datapoints[:,1], s = 30, color = 'r',label = \"Datapoints\")\nplt.scatter(test_pt[0],test_pt[1], s = 35, label = \"Point p\")\nplt.xlim(2,4)\nplt.ylim(1,3)\nplt.legend(loc = 2)\nplt.title(\"Density-Reachable with radius 0.50\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## DBSCAN\n\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise) is the most well-known density-based clustering algorithm.\n\nUnlike K-Means, DBSCAN does not require the number of clusters as a parameter. Rather it infers the number of clusters based on the data, and it can discover clusters of arbitrary shape (for comparison, K-Means usually discovers spherical clusters). As I said earlier, the ɛ-neighborhood is fundamental to DBSCAN to approximate local density, so the algorithm has two parameters:\n\n1. ɛ: The radius of our neighborhoods around a data point p.\n2. minPts: The minimum number of data points we want in a neighborhood to define a cluster.\n\nUsing these two parameters, DBSCAN categories the data points into three categories:\n\n- `Core Points`: A data point p is a core point if Nbhd(p,ɛ) [ɛ-neighborhood of p] contains at least minPts ; | Nbhd(p,ɛ) | >= minPts.\n\n- `Border Points`: A data point *q is a border point if Nbhd(q, ɛ) contains less than minPts data points, but q is reachable from some core point p.\n\n- `Outlier`: A data point o is an outlier if it is neither a core point nor a border point. Essentially, this is the \"other\" class.\n\nThese definitions may seem abstract, so let's cover what each one means in more detail.\n\n### CORE POINTS\n\nCore Points are the foundations for DBSCAN clusters and are based on the density approximation. We use the same ɛ to compute the neighborhood for each point, so the volume of all the neighborhoods is the same. However, the number of other points in each neighborhood is what differs. Recall that we can think of the number of data points in the neighborhood as its mass. The volume of each neighborhood is constant, and the mass of neighborhood is variable, so by putting a threshold on the minimum amount of mass needed to be core point, we are essentially setting a minimum density threshold. Therefore, core points are data points that satisfy a minimum density requirement. Our clusters are built around our core points (hence the core part), so by adjusting our minPts parameter, we can fine-tune how dense our clusters cores must be.\n\n### BORDER POINTS\n\nBorder Points are the points in our clusters that are not core points. In the definition above for border points, we used the term density-reachable. we have not defined this term yet, but the concept is simple. To explain this concept, let's revisit our neighborhood example with epsilon = 0.15. Consider the point r (the black dot) that is outside of the point p's neighborhood.\n\n<img src=\"images/dbscan-1.png\" alt=\"image1\" style=\"width: 600px;height: 400px\"/>\n\n\nAll the points inside the point p's neighborhood are said to be directly reachable from p. Now, let's explore the neighborhood of point q, a point directly reachable from p. The yellow circle represents q's neighborhood.\n\n\n<img src=\"images/neighborhood_smaller.png\" alt=\"image1\" style=\"width: 600px;height: 400px\"/>\n\n\nNow while our target point r is not our starting point p's neighborhood, it is contained in the point q's neighborhood. This is the idea behind density-reachable: If we can get to the point r by jumping from neighborhood to neighborhood, starting at a point p, then the point r is density-reachable from the point p.\n\n<img src=\"images/neighborhood_reachable.png\" alt=\"image1\" style=\"width: 600px;height: 400px\"/>\n\n\nAs an analogy, we can think of density-reachable points as being the \"friends of a friend\". If the directly-reachable of a core point p are its \"friends\", then the density-reachable points, points in neighborhood of the \"friends\" of p, are the \"friends of its friends\". One thing that may not be clear is density-reachable points is not limited to just two adjacent neighborhood jumps. As long as you can reach the point doing \"neighborhood jumps\", starting at a core point p, that point is density-reachable from p, so \"friends of a friend of a friend ... of a friend\" are included as well.\n\nIt is important to keep in mind that this idea of density-reachable is dependent on our value of ɛ. By picking larger values of ɛ, more points become density-reachable, and by choosing smaller values of ɛ, less points become density-reachable.\n\n\n## OUTLIERS\n\nFinally, we get to our \"other\" class. Outliers are points that are neither core points nor are they close enough to a cluster to be density-reachable from a core point. Outliers are not assigned to any cluster and, depending on the context, may be considered anomalous points.\n\n\n## APPLICATION\n\nDBSCAN is avaible in Scikit-Learn, and because this implementation is scalable and well-tested, we will be using it to demonstrate how DBSCAN works in practice.\n\nThe steps to the DBSCAN algorithm are:\n\n1. Pick a point at random that has not been assigned to a cluster or been designated as an `outlier`. Compute its neighborhood to determine if it's a `core point`. If yes, start a cluster around this point. If no, label the point as an `outlier`.\n2. Once we find a `core point` and thus a cluster, expand the cluster by adding all `directly-reachable` points to the cluster. Perform \"neighborhood jumps\" to find all `density-reachable` points and add them to the cluster. If an an `outlier` is added, change that point's status from `outlier` to `border point`.\n3. Repeat these two steps until all points are either assigned to a cluster or designated as an `outlier`."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "! head -10 ../data/wholesale.csv",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# DBSCAN with eps=1 and min_samples=3\n# https://archive.ics.uci.edu/ml/datasets/Wholesale+customers#\nfrom sklearn.cluster import DBSCAN\nimport pandas as pd\n\nwholesale = pd.read_csv(\"../data/wholesale.csv\")\nwholesale.drop([\"Channel\", \"Region\"], axis = 1, inplace = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "After dropping two fields that identify the customer, we can examine the first few rows of this dataset."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "wholesale.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "wholesale.head(5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "So we can visualize the data, we are going to use only two of these attributes:\n\n- `Groceries`: The customer's annual spending (in some monetary unit) on grocery products.\n- `Milk`: The customer's annual spending (in some monetary unit) on milk products."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "wholesale_selected = wholesale[[\"Grocery\", \"Milk\"]]\nwholesale_selected = wholesale.as_matrix().astype(\"float32\", copy = False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "wholesale_selected[:10]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Because the values of the data are in the thousands, we are going to normalize each attribute by scaling it to 0 mean and unit variance."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from sklearn.preprocessing import StandardScaler\n\nstdscaler = StandardScaler().fit(wholesale_selected)\nwholesale_transformed = stdscaler.transform(wholesale_selected)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, let's visualize the normalized dataset."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "plt.scatter(wholesale_transformed[:, 0], wholesale_transformed[:, 1])\nplt.xlabel(\"Groceries\")\nplt.ylabel(\"Milk\")\nplt.title(\"Wholesale Data - Groceries and Milk\")\nplt.savefig(\"wholesale.png\", format = \"PNG\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As you can see, there is positive correlation between grocery purchases and milk product purchases. There is a cluster centered about the mean milk purchase (milk = 0) and the mean groceries purchase (groceries = 0). In addition, there are a handful of outliers pertaining to customers who buy more groceries or milk products compared to other customers.\n\nWith DBSCAN, we want to identify this main cluster of customers, but we also want to flag customers with more unusual annual purchasing habits as outliers.\n\nWe will construct a DBSCAN object that requires a minimum of 15 data points in a neighborhood of radius 0.5 to be considered a core point."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "dbsc = DBSCAN(eps = .5, min_samples = 15).fit(wholesale_transformed)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, we can extract our cluster labels and outliers to plot our results."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "labels = dbsc.labels_\n\n# Return an array of zeros with the same shape and type as a given array.\ncore_samples = np.zeros_like(labels, dtype = bool)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "unique_labels = np.unique(labels)\nunique_labels",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "wholesale['cluster'] = labels\nwholesale.sort_values('cluster')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "unique_labels = np.unique(labels)\ncolors = plt.cm.Spectral(np.linspace(0,1, len(unique_labels)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "for (label, color) in list(zip(unique_labels, colors)):\n    class_member_mask = (labels == label)\n    xy = wholesale_transformed[class_member_mask & core_samples]\n    print (len(core_samples))\n    #plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor = color, markersize = 10)\n    \n    xy2 = wholesale_transformed[class_member_mask & ~core_samples]\n    #plt.plot(xy2[:,0], xy2[:,1], 'o', markerfacecolor = color, markersize = 5)\n\nplt.title(\"DBSCAN on Wholsesale data\")\nplt.xlabel(\"Grocery (scaled)\")\nplt.ylabel(\"Milk (scaled)\")\nplt.savefig(\"dbscan_wholesale.png\", format = \"PNG\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Lining up with our intuition, the DBSCAN algorithm was able to identify one cluster of customers. In addition, it was able to flag customers whose annual purchasing behavior deviated too heavily from other customers.\n\nBecause the outliers corresponded to customers with more extreme purchasing behavior, the wholesale distributor could specifically target these customers with exclusive discounts to encourage larger purchases.\n\nAs a baseline, let's run K-Means with two clusters on this dataset. The big blue dot represents the centroid for the black cluster, and the big gold dot represents the centroid for the white cluster."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters = 2).fit(wholesale_transformed)\nlabels_kmeans = kmeans.labels_\ncentroids = kmeans.cluster_centers_",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "plt.scatter(wholesale_transformed[:,0], wholesale_transformed[:,1], c = labels_kmeans)\nplt.scatter(centroids[:,0], centroids[:,1], c = [\"gold\",\"blue\"], s = 60 )\nplt.savefig(\"kmeans_wholesale.png\", format = \"PNG\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "While the white clusters appears to capture most of the outliers, the cluster basically captures customers who purchase relatively more goods. If we designate the white cluster as the \"anomalous\" cluster, then we basically flag any customer who purchases a lot of milk or groceries. If you were the wholesale distributor, then you would be calling your better customers, the ones whom you make more money from, anomalies."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## BEER EXAMPLE CONTINUED"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# beer dataset\nimport pandas as pd\nurl = '../data/beer.txt'\nbeer = pd.read_csv(url, sep=' ')\nX = beer.drop('name', axis=1)\n\n# center and scale the data\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# DBSCAN with eps=1 and min_samples=3\nfrom sklearn.cluster import DBSCAN\ndb = DBSCAN(eps=3, min_samples=3)\ndb.fit(X_scaled)",
      "execution_count": 2,
      "outputs": [
        {
          "data": {
            "text/plain": "DBSCAN(algorithm='auto', eps=3, leaf_size=30, metric='euclidean',\n    metric_params=None, min_samples=3, n_jobs=1, p=None)"
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "beer_scaled = pd.DataFrame(X_scaled, columns=beer.columns[1:])\n\n# review the cluster labels\ndb.labels_",
      "execution_count": 3,
      "outputs": [
        {
          "data": {
            "text/plain": "array([ 0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0], dtype=int64)"
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "beer['cluster'] = db.labels_\nbeer.sort_values('cluster')\nbeer.groupby('cluster').mean()",
      "execution_count": 4,
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>calories</th>\n      <th>sodium</th>\n      <th>alcohol</th>\n      <th>cost</th>\n    </tr>\n    <tr>\n      <th>cluster</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>-1</th>\n      <td>157.000000</td>\n      <td>15.000000</td>\n      <td>0.900000</td>\n      <td>0.480000</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>131.263158</td>\n      <td>14.947368</td>\n      <td>4.415789</td>\n      <td>0.495263</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "           calories     sodium   alcohol      cost\ncluster                                           \n-1       157.000000  15.000000  0.900000  0.480000\n 0       131.263158  14.947368  4.415789  0.495263"
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "beer",
      "execution_count": 5,
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>calories</th>\n      <th>sodium</th>\n      <th>alcohol</th>\n      <th>cost</th>\n      <th>cluster</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Budweiser</td>\n      <td>144</td>\n      <td>15</td>\n      <td>4.7</td>\n      <td>0.43</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Schlitz</td>\n      <td>151</td>\n      <td>19</td>\n      <td>4.9</td>\n      <td>0.43</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Lowenbrau</td>\n      <td>157</td>\n      <td>15</td>\n      <td>0.9</td>\n      <td>0.48</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Kronenbourg</td>\n      <td>170</td>\n      <td>7</td>\n      <td>5.2</td>\n      <td>0.73</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Heineken</td>\n      <td>152</td>\n      <td>11</td>\n      <td>5.0</td>\n      <td>0.77</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Old_Milwaukee</td>\n      <td>145</td>\n      <td>23</td>\n      <td>4.6</td>\n      <td>0.28</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Augsberger</td>\n      <td>175</td>\n      <td>24</td>\n      <td>5.5</td>\n      <td>0.40</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Srohs_Bohemian_Style</td>\n      <td>149</td>\n      <td>27</td>\n      <td>4.7</td>\n      <td>0.42</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Miller_Lite</td>\n      <td>99</td>\n      <td>10</td>\n      <td>4.3</td>\n      <td>0.43</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Budweiser_Light</td>\n      <td>113</td>\n      <td>8</td>\n      <td>3.7</td>\n      <td>0.40</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Coors</td>\n      <td>140</td>\n      <td>18</td>\n      <td>4.6</td>\n      <td>0.44</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Coors_Light</td>\n      <td>102</td>\n      <td>15</td>\n      <td>4.1</td>\n      <td>0.46</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Michelob_Light</td>\n      <td>135</td>\n      <td>11</td>\n      <td>4.2</td>\n      <td>0.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Becks</td>\n      <td>150</td>\n      <td>19</td>\n      <td>4.7</td>\n      <td>0.76</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Kirin</td>\n      <td>149</td>\n      <td>6</td>\n      <td>5.0</td>\n      <td>0.79</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>Pabst_Extra_Light</td>\n      <td>68</td>\n      <td>15</td>\n      <td>2.3</td>\n      <td>0.38</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>Hamms</td>\n      <td>139</td>\n      <td>19</td>\n      <td>4.4</td>\n      <td>0.43</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>Heilemans_Old_Style</td>\n      <td>144</td>\n      <td>24</td>\n      <td>4.9</td>\n      <td>0.43</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>Olympia_Goled_Light</td>\n      <td>72</td>\n      <td>6</td>\n      <td>2.9</td>\n      <td>0.46</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>Schlitz_Light</td>\n      <td>97</td>\n      <td>7</td>\n      <td>4.2</td>\n      <td>0.47</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "                    name  calories  sodium  alcohol  cost  cluster\n0              Budweiser       144      15      4.7  0.43        0\n1                Schlitz       151      19      4.9  0.43        0\n2              Lowenbrau       157      15      0.9  0.48       -1\n3            Kronenbourg       170       7      5.2  0.73        0\n4               Heineken       152      11      5.0  0.77        0\n5          Old_Milwaukee       145      23      4.6  0.28        0\n6             Augsberger       175      24      5.5  0.40        0\n7   Srohs_Bohemian_Style       149      27      4.7  0.42        0\n8            Miller_Lite        99      10      4.3  0.43        0\n9        Budweiser_Light       113       8      3.7  0.40        0\n10                 Coors       140      18      4.6  0.44        0\n11           Coors_Light       102      15      4.1  0.46        0\n12        Michelob_Light       135      11      4.2  0.50        0\n13                 Becks       150      19      4.7  0.76        0\n14                 Kirin       149       6      5.0  0.79        0\n15     Pabst_Extra_Light        68      15      2.3  0.38        0\n16                 Hamms       139      19      4.4  0.43        0\n17   Heilemans_Old_Style       144      24      4.9  0.43        0\n18   Olympia_Goled_Light        72       6      2.9  0.46        0\n19         Schlitz_Light        97       7      4.2  0.47        0"
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}