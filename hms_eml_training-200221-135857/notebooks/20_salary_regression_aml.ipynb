{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# END-TO-END MODEL DEVELOPMENT, DEPLOYMENT TO CONSUMPTION\n\nWe will build and deploy a machine model to predict the salary from the Stackoverflow dataset. By the end of this, you will be able to invoke a RESTful web service to get the predictions.\n\nSince the objective to demonstrate the workflow, we will use a simple two-column dataset with years of experience and salary for the experiment. For the details on the dataset, refer to my previous article on linear regression.\n\n## Prerequisites\n1. Basic knowledge of Python and Scikit-learn\n2. Active Microsoft Azure Subscription\n3. Anaconda or Miniconda"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Configuring the Development Environment\n\nConfigure a virtual environment with the Azure ML SDK. Run the below commands to install the Python SDK, and launching a Jupyter Notebook. Start a new Python 3 kernel from Jupyter.\n\n~~~\n \n$ pip install azureml-sdk[notebooks]\n \n~~~"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Initializing Azure ML Environment\n\nLet’s start by importing all the required Python modules, which include standard Scikit-learn modules and the Azure ML modules."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import datetime\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.externals import joblib\n \nimport azureml.core\nfrom azureml.core import Workspace\nfrom azureml.core.model import Model\nfrom azureml.core import Experiment\nfrom azureml.core.webservice import Webservice\nfrom azureml.core.image import ContainerImage\nfrom azureml.core.webservice import AciWebservice\nfrom azureml.core.conda_dependencies import CondaDependencies",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We need to create an Azure ML Workspace that acts as the logical boundary for our experiment. A Workspace creates a Storage Account for storing the dataset, a Key Vault for secrets, a Container Registry for maintaining the image repositories, and Application Insights for logging the metrics.\n\nDon’t forget to replace the placeholder with your subscription id."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "After a few minutes, we will see the resources created within the Workspace."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create a new workspace in future using the code below\n#ws = Workspace.create(name='exml01-student99',\n#                      subscription_id='', \n#                      resource_group='eml-training',\n#                      create_resource_group=True,\n#                      location='southcentralus'\n#                     )",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.authentication import AzureCliAuthentication\n\ncli_auth = AzureCliAuthentication()\n\nws = Workspace(subscription_id=\"3c3bb71f-3a4c-436f-9e0a-7407d75a82fa\",\n               resource_group=\"eml-training\",\n               workspace_name=\"eml01-student99\",\n               auth=cli_auth)\n\nprint(\"Found workspace {} at location {}\".format(ws.name, ws.location))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can now create an Experiment to start logging the metrics. Since we don’t have many parameters to log, we are capturing the start time of the training process."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "exp = Experiment(workspace=ws, name='salexp')\nrun = exp.start_logging()                   \nrun.log(\"Experiment start time\", str(datetime.datetime.now()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Training and Testing the Scikit-learn ML Model\n\nWe will now proceed to train and test the model through Scikit-learn."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "sal = pd.read_csv('../data/sal.csv',header=0, index_col=None)\nX = sal[['x']]\ny = sal['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=10)\n \nlm = LinearRegression()\nlm.fit(X_train,y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The trained model will be serialized as a pickle file in the outputs directory. Azure ML automatically copies the content of the outputs directory to the cloud."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "filename = 'outputs/sal_model.pkl'\njoblib.dump(lm, filename)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let’s complete the experiment by logging the slope, intercept, and the end time of the training job."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run.log('Intercept :', lm.intercept_)\nrun.log('Slope :', lm.coef_[0])\n \nrun.log(\"Experiment end time\", str(datetime.datetime.now()))\nrun.complete()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can track the metrics and the execution time from the Azure Dashboard."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Registering and Serving the Trained Model\n\nEach time we freeze the model, it can be registered with Azure ML with a unique version. This gives us the ability to easily switch between different models when serving.\n\nLet’s register the salary model from the above training job by pointing the SDK to the location of the PKL file. We are also adding some additional metadata to the model in the form of tags."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model = Model.register(model_path = \"outputs/sal_model.pkl\",\n                       model_name = \"sal_model\",\n                       tags = {\"key\": \"1\"},\n                       description = \"Salary Prediction\",\n                       workspace = ws)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Check the Models section of the Workspace to ensure that our model is registered."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "It’s time for us to package and deploy the model as a container image which will be exposed as a web service.\n\nFor the container image to get created, we need to tell Azure ML about the environment needed by the model. We will then pass a Python script that includes code to predict the values based on an inbound data point.\n\nAzure ML API provides handy methods for both. Let’s first create the environment file, salenv.yaml, which tells the runtime to include Scikit-learn in the container image."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "salenv = CondaDependencies()\nsalenv.add_conda_package(\"scikit-learn\")\n \nwith open(\"salenv.yml\",\"w\") as f:\n    f.write(salenv.serialize_to_string())\n\nwith open(\"salenv.yml\",\"r\") as f:\n    print(f.read())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The below snippet, when executed from the Jupyter Notebook, creates a file called score.py that contains the inference logic for the model."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile score.py\n \nimport json\nimport numpy as np\nimport os\nimport pickle\nfrom sklearn.externals import joblib\nfrom sklearn.linear_model import LogisticRegression\n \nfrom azureml.core.model import Model\n \ndef init():\n    global model\n    # retrieve the path to the model file using the model name\n    model_path = Model.get_model_path('sal_model')\n    model = joblib.load(model_path)\n\ndef run(raw_data):\n    data = np.array(json.loads(raw_data)['data'])\n    # make prediction\n    y_hat = model.predict(data)\n    return json.dumps(y_hat.tolist())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now let’s connect the dots by passing the inference file and environment configuration to the image."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%time\n \nimage_config = ContainerImage.image_configuration(execution_script=\"score.py\", \n                                                  runtime=\"python\", \n                                                  conda_file=\"salenv.yml\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This eventually results in the creation of a container image which shows up in the Images section of the Workspace."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We are all set to create the deployment configuration that defines the target environment and launching it as web service hosted in Azure Container Instance as a single-vm container. We may also choose AKS or an IoT Edge environment as the deployment target."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n                                               memory_gb=1, \n                                               tags={\"data\": \"Salary\",  \"method\" : \"sklearn\"}, \n                                               description='Predict Stackoverflow Salary')\n \nservice = Webservice.deploy_from_model(workspace=ws,\n                                       name='salary-svc03',\n                                       deployment_config=aciconfig,\n                                       models=[model],\n                                       image_config=image_config)\n \nservice.wait_for_deployment(show_output=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The Azure Resource Group now has an Azure Container Instance running the inference for the model."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can get the URL of the inference service from the below method:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(service.scoring_uri)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let’s go ahead and invoke with the web service programatically. We can do this from the same Jupyter Notebook."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import requests\nimport json\n\n# URL for the web service\nscoring_uri = service.scoring_uri\n\n# If the service is authenticated, set the key\n#key = '<your key>'\n\n# Two sets of data to score, so we get two results back\ndata = { \"data\": [[45]] }\n\n# Convert to JSON string\ninput_data = json.dumps(data)\n\n# Set the content type\nheaders = { 'Content-Type':'application/json' }\n\n# If authentication is enabled, set the authorization header\n#headers['Authorization']=f'Bearer {key}'\n\n# Make the request and display the response\nresp = requests.post(scoring_uri, input_data, headers = headers)\nprint(resp.text)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The uniqueness of this approach is that we could perform all the tasks from a Python kernel running inside the Jupyter Notebook. Developers can do everything it takes to train and deploy ML models from code. This is the real value of using an ML PaaS like Azure ML Service."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}