{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# CROSS-VALIDATION FOR PARAMETER TUNING, MODEL SELECTION, AND FEATURE SELECTION\n\n## OVERVIEW\n- What is the drawback of using the **train/test split** procedure for model evaluation?\n- How does **K-fold cross-validation** overcome this limitation?\n- How can cross-validation be used for selecting **tuning parameters**, choosing between **models**, and selecting **features**?\n- What are some possible **improvements** to cross-validation?\n\n\n## REVIEW OF MODEL EVALUATION PROCEDURES \n\n**Motivation:** Need a way to choose between machine learning models\n- Goal is to estimate likely performance of a model on **out-of-sample data**\n\n**Initial idea:** Train and test on the same data\n- But, maximizing **training accuracy** rewards overly complex models which **overfit** the training data\n\n**Alternative idea:** Train/test split\n- Split the dataset into two pieces, so that the model can be trained and tested on **different data**\n\n**Testing accuracy** is a better estimate than training accuracy of out-of-sample performance\n- But, it provides a **high variance** estimate since changing which observations happen to be in the testing set can significantly change testing accuracy\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\nimport seaborn as sn\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# read in the iris data\niris = load_iris()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# create X (features) and y (response)\nX = iris.data\ny = iris.target",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# use train/test split with different random_state values\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=4)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# check classification accuracy of KNN with K=5\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nprint (metrics.accuracy_score(y_test, y_pred))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "- **Question:** What if we created a bunch of train/test splits, calculated the testing accuracy for each, and averaged the results together?\n- **Answer:** That's the essense of cross-validation!"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### STEPS FOR K-FOLD CROSS-VALIDATION"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "- Split the dataset into K **equal** partitions (or \"folds\").\n- Use fold 1 as the **testing set** and the union of the other folds as the **training set**.\n- Calculate **testing accuracy**.\n- Repeat steps 2 and 3 K times, using a **different fold** as the testing set each time.\n- Use the **average testing accuracy** as the estimate of out-of-sample accuracy."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# simulate splitting a dataset of 25 observations into 5 folds\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=5, shuffle=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "kf",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\n\n# Create data\ndata = np.array(range(1,26))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "data",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# print the contents of each training and testing set\nprint ('{} {:^61} {}'.format('Iteration', 'Training set observations', 'Testing set observations'))\nfor iteration, data in enumerate(kf.split(data)):\n    print (\"{0:^9} {1} {2}\".format(iteration, data[0], data[1]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "- Dataset contains **25 observations** (numbered 0 through 24)\n- 5-fold cross-validation, thus it runs for **5 iterations**\n- For each iteration, every observation is either in the training set or the testing set, **but not both**\n- Every observation is in the testing set **exactly once**\n\n### 10-FOLD CROSS-VALIDATION\n\n<img src='./images/10_fold_cv.png'/>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## COMPARING CROSS-VALIDATION TO TRAIN/TEST SPLIT"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Advantages of **cross-validation:**\n- More accurate estimate of out-of-sample accuracy\n- More \"efficient\" use of data (every observation is used for both training and testing)\n\n### Advantages of **train/test split:**\n- Runs K times faster than K-fold cross-validation\n- Simpler to examine the detailed results of the testing process"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## CROSS-VALIDATION EXAMPLE\n### PART 1: PARAMETER TUNING"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Goal:** Select the best tuning parameters (aka \"hyperparameters\") for KNN on the iris dataset"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import cross_val_score",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# 10-fold cross-validation with K=5 for KNN (the n_neighbors parameter)\nknn = KNeighborsClassifier(n_neighbors=5)\nscores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\nprint (scores)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# use average accuracy as an estimate of out-of-sample accuracy\nprint (scores.mean())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# search for an optimal value of K for KNN\nk_range = range(1, 31)\nk_scores = []\nk_score_items = []\n\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n    k_score_items.append(scores)\n    k_scores.append(scores.mean())\nprint (k_scores)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "for x, y in enumerate(k_scores):\n    print (x+1,y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "len(k_scores)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "KNeighborsClassifier?",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# plot the value of K for KNN (x-axis) versus the cross-validated accuracy (y-axis)\nimport matplotlib.pyplot as plt\nplt.plot(k_range, k_scores)\nplt.xlabel('Value of K for KNN')\nplt.ylabel('Cross-Validated Accuracy')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### PART 2: MODEL SELECTION\n**Goal:** Compare the best KNN model with logistic regression on the iris dataset"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# create X (features) and y (response)\nX = iris.data\ny = iris.target",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# 10-fold cross-validation with the best KNN model\nknn = KNeighborsClassifier(n_neighbors=20)\ncross_val_score(knn, X, y, cv=10, scoring='accuracy').mean()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# 10-fold cross-validation with logistic regression\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nprint (cross_val_score(logreg, X, y, cv=10, scoring='accuracy').mean())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### PART 3: FEATURE SELECTION \n**Goal**: Select whether the Newspaper feature should be included in the linear regression model on the advertising dataset\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# read in the advertising dataset\ndata = pd.read_csv('../data/advertising.csv', index_col=0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# create a Python list of three feature names\nfeature_cols = ['TV', 'Radio', 'Newspaper']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# use the list to select a subset of the DataFrame (X)\nX = data[feature_cols]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# select the Sales column as the response (y)\ny = data.Sales",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# 10-fold cross-validation with all three features\nlm = LinearRegression()\nscores = cross_val_score(lm, X, y, cv=10, scoring='neg_mean_squared_error')\nprint (scores)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# fix the sign of MSE scores\nmse_scores = -scores\nprint (mse_scores)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# convert from MSE to RMSE\nrmse_scores = np.sqrt(mse_scores)\nprint (rmse_scores)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# calculate the average RMSE\nprint (rmse_scores.mean())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# 10-fold cross-validation with two features (excluding Newspaper)\nfeature_cols = ['TV', 'Radio']\nX = data[feature_cols]\nprint (np.sqrt(-cross_val_score(lm, X, y, cv=10, scoring='neg_mean_squared_error')).mean())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "lm.fit(X, y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "lm.predict([[100, 200]])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## IMPROVEMENTS TO CROSS-VALIDATION\n\n**Repeated cross-validation**\n- Repeat cross-validation multiple times (with **different random splits** of the data) and average the results\n- More reliable estimate of out-of-sample performance by **reducing the variance** associated with a single trial of cross-validation\n\n**Creating a hold-out set**\n- \"Hold out\" a portion of the data **before** beginning the model building process\n- Locate the best model using cross-validation on the remaining data, and test it **using the hold-out set**\n- More reliable estimate of out-of-sample performance since hold-out set is **truly out-of-sample**\n\n**Feature engineering and selection within cross-validation iterations**\n- Normally, feature engineering and selection occurs **before** cross-validation\n- Instead, perform all feature engineering and selection **within each cross-validation iteration**\n- More reliable estimate of out-of-sample performance since it **better mimics** the application of the model to out-of-sample data\n"
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}